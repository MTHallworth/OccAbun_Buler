{
    "contents" : "Modeling abundance and occupancy using point count data\n========================================================\n\n#### prepared by Michael T. Hallworth  \n#### Smithsonian Conservation Biology Institute : Migratory Bird Center\n\nPoint count data are a cost effective way of surveying large areas to answer ecological questions. For example, point count data can answer the questions: \n* How many species are present in a given area?    \n* How many individuals of each species are there?  \n* How has the community changed over time?  \n* Which habitats is X species associated with?  \n\nThese are just a few of the possible questions that can be answered using point count data. However, the question of interest determines how the data should be analyzed. If the question deals with *how many species are present here?* then using occupancy modeling would be a good fit. If the question is something like *How has the population size of Ovenbirds changed over time?*, then modeling abundance is more appropriate.\n\nRegardless of the question - accounting for imperfect detection is important. Failing to account for imperfect detection biases estimates of either species richness or abundance low - unless all species and/or individuals are detected perfectly. This is unlikely given that the study organisms are animals and move, may have been siting on a nest during your visit, preening instead of singing, etc.  \n\nThe subsequent code illustrates how to use point count data to answer ecological questions while accounting for imperfect detection. In this tutorial we will use simulated data using 373 survey locations within Hubbard Brook Experimental Forest, NH that were visited three times during the 2015 breeding season. Three, 10 minute point count surveys were conducted along 15 transects spaced 500m apart. Each survey location wasseperated by either 100m, or 200m. All species seen or heard within 50m and 100m of the point during the 10 min count were recorded.\n\n```{r}\n# CREATE A MAP SHOWING THE SURVEY LOCATIONS HERE \n```\n\n### The data\n\nRead in the data and look at the first 5 rows and the names of the columns  \n\n```{r set-options, cache=FALSE}\noptions(width = 100)\n# read in Point-count data #\nVWdata<-read.csv(\"Data/HBEF_2015_simulated.csv\",sep=\",\")\n\n# view the column names #\nnames(VWdata)\n\n# View the first 5 rows of data #\nhead(VWdata)\n```\n\nNotice that the number of each species is not in the data per se. Each time a species was encountered within the survey it was recorded using it's alpha code.  \n* OVEN = Ovenbird  \n* BTBW = Black-throated blue warbler, etc.  \n\nTo perform the analyses we need the number of individuals of each species detected within each survey.\n\n***\n> #### Exercise:\n> From the data you can see above: \n> * how many individuals of each species were there?  \n> * how would/did you determine that?  \n\n> Start thinking about what the script might look like to collate those data\n\n***\nLet's load the packages we will need before we go much further.\n\n```{r echo=FALSE}\n.libPaths(\"G:/Google_Drive/Hallworth_R_Library\")\n```\n```{r,warning=FALSE,message=FALSE,cache=TRUE}\n# load packages #\nlibrary(unmarked)\nlibrary(raster)\nlibrary(rgeos)\nlibrary(chron)\nlibrary(abind)\nlibrary(R2jags)\n```\n\n#### Brief overview of the subsequent code\nThe following code will:  \n* Clean up the data (there are little things in all data sets that need to be fixed)\n* Generate an array of count data for each survey location, period and replicate   \n* Generate an array of observation covariates - Date, Time, Observer   \n* Generate an array of location covariates - Elevation, Slope, Aspect\n\n#### Cleaning the data \n\nSome of the alpha codes have new lines (which show up in the data like \"BTBW\\n\") - if they are not fixed before the analysis \"BTBW\\n\" and \"BTBW\" will be considered different species. We need to fix this before going any further.\n```{r echo=FALSE}\nlevels(VWdata$Species)\n```\n\n```{r}\n# Some of the species codes have new lines for example \"BTBW\\n\" \n# this next code removes that & turns \"BTBW\\n\" into \"BTBW\"\n# gsub - looks for a pattern and if found substitutes with a new pattern.\n# it's looking for \"\\n\" and replacing \"\" - nothing.\n\nVWdata$Species<-as.factor(gsub(\"[\\r\\n]\", \"\", VWdata$Species))\n```\n```{r echo=FALSE}\nlevels(VWdata$Species)\n```\n\nLet's check for another common error in the data:\n\nLines with out a species entered\n```{r}\n# Some rows of the data may have been entered by accident and have no new species - they have blank species #\n# remove those rows from the data set #\nVWdata<-VWdata[VWdata$Species != \"\", ]\n\n# Confirm that they were removed by showing the rows that have blank for species #\n# should be 0 rows and it is #\nVWdata[which(VWdata$Species ==\"\"),]\n```\n\n### Formatting the data\n\nFirst we need to subset the data to include only the species and individuals counted within a 50m radius.\n\n```{r}\n# Use only the data collected within 50m of the point so no double counting #\nVWdata<-VWdata[VWdata$Distance==1,]\n```\n\n***\n> #### Excercise:  \n> Why did we only include individuals sampled within 50m of the survey point when we counted all within 100m?\n> * *hint* look back at the survey design detailed above.  \n>\n> Is there another way to subset the data to return the same information?\n\n***\n\nBefore we go further it's helpful to create some useful vectors that we will use later on.\n* How many counts were conducted\n* How many periods within the count were done  \n\nwe counted every individual heard/seen in 3 periods within the 10min count. Essentially we do 3 -3:20min point counts within the 10m count period.\n+ 1) 0-3:20 min  \n+ 2) 3:20-6:40 min \n+ 3) 6:40-10 min\n\n```{r}\n# Determine the Number of counts\nreplicate<-min(VWdata$Replicate,na.rm=TRUE):max(VWdata$Replicate,na.rm=TRUE)\n```\n```{r echo=FALSE}\nreplicate\n```\n```{r}\n# Determine the number of periods within a count \n \nperiod<-min(VWdata$Period,na.rm=TRUE):max(VWdata$Period,na.rm=TRUE)\n```\n```{r echo=FALSE}\nperiod\n```\n```{r}\n# number of sites surveyed #\n\nSites<-length(unique(VWdata[,2]))\n\n# We also need a vector of sites so we can loop through them later.\n# The `as.factor` sets the range between 1 and the number of sites surveyed.\n# this is necessary because the plots are not numbered sequentially - as.factor fixes that.\n\nsites<-levels(as.factor(VWdata[,2]))\n```\n```{r echo=FALSE}\nSites\n```\n```{r}\n# List the species of interest (S.O.I) #\nS.O.I <- c(\"BLBW\", \"BTBW\", \"BTNW\",\"OVEN\", \"REVI\")\n```\n\n### Structuring the data into capture histories\n\nNext we generate a capture history for each species - I provide an example for 1 species. In the code below we utilize `for` loops. `for` loops are used for repeated tasks. The fastest way to use `for` loops is to create an object first to store the results of the `for` loop. We do this below. \n\n```{r cache=TRUE}\n# Create a Plot x Period x Replicate array of abundance for the species of interest\n# this makes an empty 3 dimensional array - Plot x Period x Replicate   \n# that is filled with NAs \n\n# create an empty array (filled with NA) -     \n# the number of rows = sites  \n# the number of columns = periods  \n# the number of dimensions = replicates  \n\n# we will call the array a species matrix or spec.mat for short. We will want to keep track of the species so we\n# include the alpha code as well. \n\nspec.mat_OVEN<- array(NA, c(length(sites), length(period),length(replicate)))\nrownames(spec.mat_OVEN)<-sites\n\n# This next line makes the spec.mat for each species. \n\nspec.mat_BLBW<-spec.mat_BTBW<-spec.mat_BTNW<-spec.mat_REVI<-spec.mat_OVEN\n\n# Here is how to fill the array with the data that we want # \n# Here is an example for the Ovenbird #\n# OVEN \n\ntemp.sp <- subset(VWdata, Species == S.O.I[4]) # SET S.O.I number to SPECIES OF INTEREST - 4 is OVEN\n\n# for each replicate #\n  for(i in replicate){\n    # this subsets for each replicate \n      temp.rp <- subset(temp.sp, Replicate == i)\n# for each period # \n   # within each replicate - subset by period \n      for (p in period){\n           temp.p<- subset(temp.rp,Period==p)\n# for each plot #\n       for(k in sites){\n         \n    \t\t   temp.st <- subset(temp.p, Plot == k)\n    \t\t   temp.abund <- as.numeric(length(temp.st$Species))\n       \t   spec.mat_OVEN[k,p,i+1]<- temp.abund     # HERE YOU NEED TO CHANGE THE spec.mat NAME #\n           \n\t\t}  # For plot\n\t}  # For period\n}# For Replicate\n```\n   \nLet's take a look at what the resulting array looks like. The `str` function stands for structure and can be used to see what the structure of an object looks like. \n   \n```{r}\n# Here you can see that we have 373 rows - 1 for each survey site,\n# 3 columns - 1 for each period\n# and the 3rd dimenson represents the replicate.\n\nstr(spec.mat_OVEN)\n\n#let's take a look at the first few rows for replicate 1\n\nhead(spec.mat_OVEN[,,1])\n```\n\nThere were no Ovenbirds detected on the first few survey points during the first replicate. \n\n### Covariates \n**Site level Covariates**\n\nThe next bit of code extracts some potential covariates that we might be interested in from the data set.\n\nFirst, we will get some of the physical covariates for each site\n   \n```{r cache=TRUE,message=FALSE}\n##########################################################################################\n#\n# Generate the physical covariates for each site \n#\n##########################################################################################\n\n# Save the projection of the data so we can use it easily later #\n\nUTM19N<-\"+proj=utm +zone=19 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n\n# Read in HBEF boundary #\n\nHBEF<-shapefile(\"Spatial_Layers/HBEFboundary_WGS84.shp\")\n\n# Project the shapefile into UTMs \n\nHBEF<-spTransform(HBEF,CRS(UTM19N))\n\n# Read in site locations \n\nSiteLocs<-shapefile(\"Spatial_Layers/SchwarzPlots_WGS84.shp\")\n\n# keep only the Plots were we actually count #\n\nSiteLocs<-SiteLocs[which(SiteLocs$SCHWARZ_ID %in% sites),]\n\n# Transform the data into UTMs\nSiteLocs<-spTransform(SiteLocs,CRS(UTM19N))\n\n# Read in digital elevation model to extract elevation # \n\nDEM<-raster(\"Spatial_Layers/hb10mdem.txt\")\n\n# Project the data into UTMs\nDEM<-projectRaster(DEM,crs=UTM19N)\n\n# Create Slope and Aspect because it might be important #\n\nAspect<-terrain(DEM,'aspect')\n\nSlope<-terrain(DEM,'slope')\n\n```\n```{r echo=FALSE,warning=FALSE,message=FALSE,cache=TRUE,fig.height=6,fig.width=10}\nHS<-raster::hillShade(Aspect,Slope)\nHS<-mask(HS,HBEF)\nDEM<-mask(DEM,HBEF)\npar(mar=c(0,0,0,0),bty=\"n\")\nplot(HS,col = grey(0:100/100), legend = FALSE,axes=FALSE)\nplot(DEM,add=TRUE,col=terrain.colors(60),alpha=0.7,legend=FALSE)\nplot(SiteLocs,add=TRUE,pch=19,cex=0.5)\nplot(HBEF,add=TRUE,border=\"black\",lwd=2)\n```\n\n\n#### Extract physical characteristics to each survey site \n\nOften times when modeling either abundance or occupancy - it helps to standardize the covariates in the model. This helps the model converge faster. Below we will make two `data.frames`. One will store the covariate data, the other will store the standardized covariates. \n\n```{r cache=TRUE}\n# Make an empty data.frame to store the covariates #\n# here we make two data.frames - one to store the Covariates and one to store standardized covariates (siteCovs) #\n\nCovariates<-siteCovs<-data.frame(Elevation=rep(NA,length(sites)),\n                                 Aspect=rep(NA,length(sites)),\n                                 Slope=rep(NA,length(sites)))\n\n# Assign the site names as row names #\nrownames(siteCovs)<-sites\nrownames(Covariates)<-sites\n\n# Elevation \n\nCovariates[,1]<-extract(DEM,SiteLocs)\nsiteCovs[,1]<-(Covariates[,1]-mean(Covariates[,1]))/sd(Covariates[,1])\n\n# Aspect\n\nCovariates[,2]<-extract(Aspect,SiteLocs)\nsiteCovs[,2]<-(Covariates[,2]-mean(Covariates[,2]))/sd(Covariates[,2])\n\n# Slope\n\nCovariates[,3]<-extract(Slope,SiteLocs)\nsiteCovs[,3]<-(Covariates[,3]-mean(Covariates[,3]))/sd(Covariates[,3])\n```\n```{r echo=FALSE}\nhead(Covariates);head(siteCovs)\n```\nNow that we have the physical site covariates we need to create survey or observation covariates.\n\n#### Survey / Observation Covariates\n\nHere we set up covariates that could influence detection. For example - the time a point count started may influence detection - we may want to include that in our model. Below is code showing how to set up these covariates from the count data. \n\n#### date \n```{r cache=TRUE}\n\n# Format the dates\n\nVWdata$Date2 <- as.Date(VWdata$Date, format=\"%m/%d/%Y\")\n\n# create Day of year (doy) variable - Julian date\n\nJan1.15 <- as.Date(\"1/1/2015\", format=\"%m/%d/%Y\")\n\nJan1.15 <- sapply(Jan1.15, as.integer)\n\nVWdata$doy <- sapply(VWdata$Date2, as.integer)\n\nVWdata$doy <- VWdata$doy - Jan1.15\n\n# Create a site x occasion matrix of dates    \n\ndoy.15 <- matrix(NA, length(sites), length(replicate))\nrownames(doy.15) <- sites\ncolnames(doy.15) <- replicate\n\nfor(k in Sites){ \n  for(i in replicate) {\n    temp <- subset(VWdata, Replicate==i & Plot==k)\n    temp.DOY <- as.numeric(levels(factor(temp$doy)))\n    if (length(temp.DOY)>0){\n        doy.15[k,i+1] <- temp.DOY\n        } \n    } # For replicate\n} # For Sites\n\n# Change the matrix from characters to numeric #\n\ndoy.15<-structure(as.numeric(doy.15), dim=dim(doy.15), dimnames=dimnames(doy.15))\n\n# All covariates need to be standardized. You can use the function scale() to do that #\n\ndoy.15scaled<-scale(doy.15)\n\n# Any missing values can be given the mean very easily after we standardize. The mean is now 0. \n# Fill any missing values with the mean.\n\ndoy.15scaled[is.na(doy.15scaled)]<-0\n```\n  \n#### Observer  \n\n```{r cache=TRUE}\n# looks at the different observers #\nlevels(factor(VWdata$Observer))\n\n# Create a site x occasion matrix of obs \nobs.15 <- matrix(NA, length(sites), 4)\nrownames(obs.15) <- sites\nfor(k in Sites){\nfor(i in replicate) {\n    temp <- subset(VWdata, Replicate==i & Plot==k)\n    temp.obs <- temp$Observer\n    if(length(temp.obs) > 0) {\n        obs.15[k,i+1] <- unique(temp.obs)\n        } #If\n    } # For Sites\n} # For replicate\nobs.15[is.na(obs.15)]<-0\n```\n\n#### Time of count \n\n```{r cache=TRUE}\n# Format the times\n\nVWdata$Time2 <-gsub(\" AM\", \"\", VWdata[,5])\nhead(VWdata$Time2)\n# Create a site x occasion matrix of times \n\ntime.15 <- matrix(NA, length(sites), length(replicate))\nrownames(time.15 ) <- sites\ncolnames(time.15)<-replicate\n\nfor(k in Sites){\nfor(i in replicate){\n    temp <- subset(VWdata, Replicate==i & Plot==k)\n    temp.time <- temp$Time2\n    if(length(temp.time) > 0) {\n        time.15[k,i+1] <- unique(temp.time)\n        } # If \n    } # For Sites\n} # For replicate\n\n# Change the matrix from characters to numeric #\ntimes.15<-structure(times(time.15), dim=dim(time.15), dimnames=dimnames(time.15))\n\n# Scale the time covariate #\ntime<-scale(times.15)\n\n# Fill missing data with the mean - which is 0 after being scaled #\ntime[is.na(time)]<-0\n```\n\n### Running the analysis using unmarked\n\nNow that we have all data structured - we can start the analysis.  \n  \nA brief overview of the subsequent code:   \n* Input the data into an `unmarked` dataframe - necessary for analysis in `unmarked`  \n* run the analysis using `unmarked`   \n* use the models to predict the number of individuals within 50x50m pixels in HBEF  \n* plot the results  \n\nWe will occupancy first, but our data are structured for abundance. First we need to covert our count data into presence/absence data. That's fairly easy to do. \n\n```{r}\n# Here we transfer the abundance data to a new array called Oven_occu\n\nOven_occu<-spec.mat_OVEN\n\n# Here we set all values >1 = 1 - presence/absence data. \n\nOven_occu[Oven_occu>1]<-1\n```\n\n#### Make an `unmarked` frame \n```{r cache=TRUE}\n# Make an empty list to store the unmarked frames in.   \n# We first specify the occupancydata, y=Oven_occu[,,replicate]\n# we then set the siteCovs as our siteCovs matrix\n\nOVEN<-vector('list',4)\nfor(i in 1:4){\nOVEN[[i]]<-unmarkedFrameOccu(y=Oven_occu[,,i],\n                               siteCovs=siteCovs)\n}\n```\n  \n**Run the analysis**  \n\n*note* - formula format - ~covariates of detection ~ covariates for occupancy in that order\n\n```{r cache=TRUE}\n# formula describing covariates of detection and abundance, in that order\nRep0<-occu(~1 ~Elevation + Aspect + Slope, OVEN[[1]],method=\"BFGS\", se=TRUE, engine=\"C\") \n               \nRep1<-occu(~1 ~Elevation + Aspect + Slope, OVEN[[2]],method=\"BFGS\", se=TRUE, engine=\"C\")\n\nRep2<-occu(~1 ~Elevation + Aspect + Slope, OVEN[[3]],method=\"BFGS\", se=TRUE, engine=\"C\")\n             \nRep3<-occu(~1 ~Elevation + Aspect + Slope, OVEN[[4]],method=\"BFGS\", se=TRUE, engine=\"C\")\n             \n```\n```{r}\nRep0\n```\n\n*** \n> #### Exercise:\n> Try to fit a model with a detection covariate. Does it improve the fit?\n\n***\n**Predictions**\n  \n```{r cache=TRUE}\n# structuring data so we can make predictions based on physical attributes \n\n# Read in 50m x 50m grid to make predictions on #\n\nHBEFfishnet<-shapefile(\"Spatial_Layers/HBEFfishnet.shp\")\nHBEFcentroids<-shapefile(\"Spatial_Layers/HBEFfishnet_label.shp\")\n\n# Calculate standardized Elevation, Aspect and Slope for predictions #\n\nSTDelev<-calc(DEM,fun=function(x){(x-mean(Covariates[,1]))/sd(Covariates[,1])})\nSTDaspect<-calc(Aspect,fun=function(x){(x-mean(Covariates[,2]))/sd(Covariates[,2])})\nSTDslope<-calc(Slope,fun=function(x){(x-mean(Covariates[,3]))/sd(Covariates[,3])})\n```\n```{r eval=FALSE}\n# Make a data set called NewData with the coordinates, Elevation, Aspect and Slope #\n# Note - the variable names need to be EXACTLY the same as the unmarked frame covariate names\n\nNewData<-data.frame(x=HBEFcentroids@coords[,1],\n                    y=HBEFcentroids@coords[,2],\n                    Elevation=extract(STDelev,HBEFfishnet,fun=mean),\n                    Aspect=extract(STDaspect,HBEFfishnet,fun=mean),\n                    Slope=extract(STDslope,HBEFfishnet,fun=mean))\n```\n```{r echo=FALSE}\nNewData<-read.csv(\"HBEFgridcovs.csv\")\n```  \n\nNow that we have set up a new data set to make predictions we can go ahead and use the output from the analysis to determine occupancy within each 50x50m area of HBEF. \n\n```{r cache=TRUE, message=FALSE,warning=FALSE}\n# compute the expected occupancy at each grid cell\n\n## extract the coefficients for the best model - we only ran one model \nbetaRep0<-coef(Rep0)\n```\n### Map the results  \n\nThere are two ways to map the results  \n* create rasters of standardize covariates and use raster calculations  \n* make a raster from the predicted occupancy  \n\nRaster calculations:\n```{r cache=TRUE}\n\n# Make the rasters for the standardize covariates\n\nelevation<-rasterFromXYZ(cbind(NewData[,c(1:2,3)]))\naspect<-rasterFromXYZ(cbind(NewData[,c(1:2,4)]))\nslope<-rasterFromXYZ(cbind(NewData[,c(1:2,5)]))\n\n# Raster calculations - the overlay function takes more than two raster layers\n# note - psi - occupancy is modeled using the logit scale - to backtransform we can \n# use the built in R function - plogis.\n\n# Make a raster of Ovenbird Occupancy #\nOVEN0<-overlay(elevation,aspect,slope,fun=function(x,y,z){\n               plogis(betaRep0[1]+ # intercept\n                      betaRep0[2]*x +  # x is the elevation layer\n                      betaRep0[3]*y +  # y is the aspect layer\n                      betaRep0[4]*z)})   # z in the slope layer \n                    \n```\n```{r echo=FALSE}\nOVEN0<-mask(OVEN0,HBEF)\n```\nPredctions using the predict function in the `unmarked` package\n```{r message=FALSE,results='hide',cache=TRUE}\nOccuOven<-predict(Rep0,type=\"state\",newdata=NewData,appendData=TRUE)\n```\n```{r}\nhead(OccuOven)\n```\nMake the map from the predictions\n```{r}\nOVEN.occu<-raster::rasterFromXYZ(cbind(OccuOven[,5:6],OccuOven[,1]))\n```\n```{r echo=FALSE}\nOVEN.occu<-raster::mask(OVEN.occu,HBEF)\n```\n```{r echo=FALSE,fig.height=5,fig.width=12,fig.align='center'}\npar(mfrow=c(1,2),mar=c(0,0,4,0),bty=\"n\")\nplot(OVEN0,col=bpy.colors(20),axes=FALSE,main=\"Raster Calc\")\nplot(OVEN.occu,,col=bpy.colors(20),axes=FALSE,main=\"Predict\")\n```\n\n## Abundance\n\nLet's use the same point count data to determine how many Ovenbirds are breeding within Hubbard Brook. We first need to create another unmarked frame for the abundance model. We will use the `pcount` function to estimate abundance. \n\nWe need to use the data we collated earlier - not the binary data we used for occupancy.\n```{r}\nstr(spec.mat_OVEN)\n```\n```{r cache=TRUE}\n# Make an empty list to store the unmarked frames in.   \n# We first specify the abundance data, y=spec.mat_OVEN[,,replicate]\n# we then set the siteCovs as our siteCovs matrix\n\nOVEN<-vector('list',4)\nfor(i in 1:4){\nOVEN[[i]]<-unmarkedFramePCount(y=spec.mat_OVEN[,,i],\n                               siteCovs=siteCovs)\n}\n```\n  \n**Run the analysis**  \n\n*note* - formula format - ~covariates of detection ~ covariates for abundance in that order\n\n```{r cache=TRUE}\n# formula describing covariates of detection and abundance, in that order\nAbun0<-pcount(~1 ~Elevation + Aspect + Slope, OVEN[[1]], K=200, mixture=c(\"P\"),\n              method=\"BFGS\", se=TRUE, engine=\"C\")\n```\n```{r cache=TRUE, message=FALSE}\n# compute the expected abundance at each grid cell\n# sum(lamnew) is the expected value of total population size\n\n## extract the coefficients for the best model\nbetaAbun0<-coef(Abun0)\n```\n\n***\n> #### Exercise:\n> Prior to N-mixture models, people used the maximum number of individuals within a count to determine abundance.   \n\n> Compare the population size of Ovenbirds that was sampled:\n> * without accounting for imperfect detection  \n> * after accounting for imperfect detection  \n \n> What do you expect to find? Why?  \n\n***\n```{r}\n# This function first finds the maximum number of Ovenbirds per count (rows), then sums them to get a raw abundance value.\n\nsum(apply(spec.mat_OVEN[,,1],1,max))\n\n# This the predict function run applies the model results to a set of data - here our site covariates. We then sum the first column of the output which is the abundance estimate. \n\nsum(predict(Abun0,type=\"state\",newdata=siteCovs,appendData=FALSE)[,1])\n```\n\nNow, let's make predictions for the entire Hubbard Brook valley. \n```{r}\n# Raster calculation \n\nAbundance0<-overlay(elevation,aspect,slope,fun=function(x,y,z){\n                  exp(betaAbun0[1]+ # intercept\n                      betaAbun0[2]*x +  # x is the elevation layer\n                      betaAbun0[3]*y +  # y is the aspect layer\n                      betaAbun0[4]*z)})   # z in the slope layer\n```\n\nHere is another way to make predictions without using the raster package.\n```{r}\nAbunPred<-predict(Abun0,type=\"state\",newdata=NewData,appendData=TRUE)\n\n```\n```{r}\n# Make a raster of Ovenbird abundance #\nAbun0<-rasterFromXYZ(cbind(AbunPred[,5:6],AbunPred[,1]))\n\n# Cut to just HBEF #\nAbun0<-mask(Abun0,HBEF)\n\nplot(Abun0)\n\n# How many Ovenbirds are there within HBEF #\ncellStats(Abun0,sum)\n```\n\n## Advanced example ###\n\n### Analysis in a Bayesian framework\n*note* additional software is required program JAGS or OpenBUGS. \n\nThe subsequent code is for doing the same analysis but in a Bayesian framework. This approach is a bit more flexible. It allows us to use the abundance during the previous count to inform the subsequent count.\n\nSpecifying the model\n```{r}\n#########################################################################################\n#########################################################################################\n#\n#         MODEL\n#\n#########################################################################################\n#########################################################################################\n\nSettlment.dm.bayes.model<-function()\n{\n##########################################################################################\n#\n#  Priors\n\n##########################################################################################\n   gam0 ~ dnorm(0, 0.01)  # intercept for t+1\n   gam1 ~ dnorm(0, 0.01)  # recruitment parameter\n   pInt ~ dnorm(0, 0.01)  # intercept for detection\n   alpha ~ dnorm(0,0.01)  # intercept for first count\n\n\n# beta estimates\n   for (c in 1:ncovs){ \n       beta[c]~dnorm(mu.theta,tau.theta)\n       betaG[c]~dnorm(mu.theta,tau.theta)\n      } #ncovs\n\n# detection\nfor (m in 1:pcovs){\n      betaP[m]~dnorm(mu.thetaP,tau.thetaP)  \n  } #pcovs\nfor(k in 1:nreplicate){\nError[k]~dnorm(0,0.01)\n}    #nyears\n\n### Hyperpriors #######\nmu.theta ~ dnorm(0,0.01)\ntau.theta ~ dgamma(0.001,0.001)\nmu.thetaP ~ dnorm(0,0.01)\ntau.thetaP ~ dgamma(0.001,0.001)\n\n##########################################################################################\n#\n#  Likelihood\n#\n##########################################################################################\nfor(i in 1:nschwarz) {                       # Schwarz Plot\n     N[i,1] ~ dpois(lambda[i,1])\n     lambda[i,1]<-exp( alpha+               #intercept\n                       beta[1]*Elev[i]+        #Elevation\n                       beta[2]*Slope[i]+       #Slope\n                       beta[3]*Aspect[i]+      #Aspect\n                       Error[1])             #Random Error term                                                         \n\n   for(j in 1:nperiod) {                        # Periods within Count\n     y[i,j,1] ~ dbin(p[i,j,1], N[i,1]) \n     p[i,j,1]<-1/(1+exp(-logit.p[i,j,1]))\n     logit.p[i,j,1]<-pInt+                     # Detection intercept\n                     betaP[1]*time[i,1]+     # time of count\n                     betaP[2]*date[i,1]+     # date of count\n                     betaP[3]*obsvr[i,1]     # observer who counted\n     } #Period\n\nfor(k in 2:nreplicate) {                      # Replicates\n     N[i,k] ~ dpois(gamma[i,k-1])\n     gamma[i,k-1] <- exp(gam0+                # intercept\n                         gam1*N[i,k-1]+       # Recruitment                                             \n    \t\t betaG[1]*Elev[i]+\n                         betaG[2]*Slope[i]+\n                         betaG[3]*Aspect[i]+\n                         Error[k])\n\n     for(j in 1:nperiod){                      # Periods within Count\n     y[i,j,k] ~ dbin(p[i,j,k], N[i,k]) \n     p[i,j,k]<-1/(1+exp(-logit.p[i,j,k]))\n     logit.p[i,j,k]<-pInt+\n                     betaP[1]*time[i,k]+\n                     betaP[2]*date[i,k]+\n                     betaP[3]*obsvr[i,k]\n     }    # Period\n   }      # Replicate\n} # Schwarz\n################################################################\n#\n# Derived parameters \n#\n################################################################\n#\n# Calculate the population size sampled at HBEF in each replicate, k.\n#\nfor(k in 1:nreplicate) {\n  Ntot[k] <- sum(N[,k])\n  }\n P <- mean(p[,,])\n\n} # END MODEL\n```\n\nWe need to give JAGS (the Gibbs Sampler) the starting values for the analysis - these are called the initial values.   \n  \n    \n```{r cache=TRUE}\n# Set initial values for N \n#\n\n# create array with same dimensions as abundance estimates\n\n# Set up data to run in parallel #\n\nnreplicate<-4\nncovs<-3\nnschwarz<-373\nnperiod<-3\npcovs<-3\n\ny<-spec.mat_OVEN\n\nElev<-siteCovs[,1]\nAspect<-siteCovs[,2]\nSlope<-siteCovs[,3]\n\ndate<-doy.15scaled\nobsvr<-obs.15\ntime<-time\n\nwin.data<-list(\"y\", \"nschwarz\", \"nreplicate\", \"nperiod\",\n              \"ncovs\",\"pcovs\",\"Elev\", \"time\", \"date\", \"obsvr\", \n              \"Slope\",\"Aspect\")\n\n# Function to put into initial values for Nst #\nN<-function(x){\nNst<-array(NA,dim=c(nschwarz,nreplicate))\nNst<-apply(x,c(1,3),max,na.rm=TRUE)+3\n# If Nst==NA change value to 3 #\nNst[Nst==-Inf]<-NA\nNst[is.na(Nst)]<-3\nreturn(Nst)\n}\n\n# setting the seed makes it so the results are the same every time  \n# it makes it so the random number generator generates the same  \n# numbers every time\nset.seed(04823)\ninits<-function() list(gam0=rnorm(1,0.01),  # sample 1 random number from a normal dist.\n                       gam1=rnorm(1,0.01),  # sample 1 random number from a normal dist.\n                       mu.theta=0,          \n                       tau.theta=10,\n                       mu.thetaP=0,\n                       tau.thetaP=10,\n                       N=N(y))\n\n# Set the number of chains to run \nnchains<-3\n\ninits.parallel<-list()\nfor(i in 1:nchains){\ninits.parallel[[i]]<-inits()\n}\n\n# Specify wihch parameters you want to monitor \nparams<-c(\"N\",\"Ntot\",\"alpha\",\"beta\",\"betaG\",\"betaP\",\"Error\",\"P\",\"gam0\",\"gam1\")\n\nset.seed(04823)\n\n# read in code that allows for parallel computation \nsource(\"jags.parallel2.r\")\n\nSys.time()\ndm.fit.settlement<-jags.parallel2(model=Settlment.dm.bayes.model, \n                                  data=win.data,\n                                  inits=inits.parallel,\n                                  parameters=params,\n                                  jags.seed=04823,\n                                  n.iter=10000,\n             \t                    n.burnin=5000,\n                                  n.thin=25,\n                                  n.chains=3,\n                                  working.directory=getwd())\nSys.time()\n```\n\nThe Bayesian anaylsis took nearly an hour on my laptop - it will likely run faster on a newer machine.  \n\n \nGet mean estimates of the following variables so we can make predictions\n```{r cache=TRUE}\nbeta<-dm.fit.settlement$BUGSoutput$mean$beta\nbetaG<-dm.fit.settlement$BUGSoutput$mean$betaG\nAlpha<-dm.fit.settlement$BUGSoutput$mean$alpha\nError<-dm.fit.settlement$BUGSoutput$mean$Error\ngam0<-dm.fit.settlement$BUGSoutput$mean$gam0\ngam1<-dm.fit.settlement$BUGSoutput$mean$gam1\n\n# Map predictions from JAGS model \n\n# Make the prediction surfaces\n\nElev_pred<-rasterFromXYZ(NewData[,1:3])\nSlope_pred<-rasterFromXYZ(NewData[,c(1:2,5)])\nAspect_pred<-rasterFromXYZ(NewData[,c(1:2,4)])\n```\n\n**Making predictions**\n```{r cache=TRUE}\n# Use the equation in the model to make predictions of round 0 #\nRound0<-overlay(Elev_pred,Aspect_pred,Slope_pred,fun=function(x,y,z){\n               exp(Alpha+beta[1]*x+beta[2]*z+beta[3]*y+Error[1])})\n\n# Include previous rounds Abundance estimate into model - NOTE (Round0) #\nRound1<-overlay(Elev_pred,Aspect_pred,Slope_pred,Round0,fun=function(x,y,z,a){\n               exp(gam0+gam1*a+betaG[1]*x+betaG[2]*z+betaG[3]*y+Error[2])})\n\n# Include previous rounds Abundance estimate into model - NOTE (Round1) #\nRound2<-overlay(Elev_pred,Aspect_pred,Slope_pred,Round1,fun=function(x,y,z,a){\n               exp(gam0+gam1*a+betaG[1]*x+betaG[2]*z+betaG[3]*y+Error[3])})\n\n# Include previous rounds Abundance estimate into model - NOTE (Round2) #\nRound3<-overlay(Elev_pred,Aspect_pred,Slope_pred,Round2,fun=function(x,y,z,a){\n               exp(gam0+gam1*a+betaG[1]*x+betaG[2]*z+betaG[3]*y+Error[4])})\n\n# Cut the estimates to only HBEF #\nRound0<-mask(Round0,HBEF)\nRound1<-mask(Round1,HBEF)\nRound2<-mask(Round2,HBEF)\nRound3<-mask(Round3,HBEF)\n```\n  \n**Calculate the change in abundance between the rounds**\n\n```{r cache=TRUE}\ndiff0_1<-overlay(Round0,Round1,fun=function(x,y){y-x})\ndiff1_2<-overlay(Round1,Round2,fun=function(x,y){y-x})\ndiff2_3<-overlay(Round2,Round3,fun=function(x,y){y-x})\ndiff0_3<-overlay(Round0,Round3,fun=function(x,y){y-x})\n\n\n# Calculate the number of OVENBIRDS # \ncellStats(Round0,sum)\ncellStats(Round1,sum)\ncellStats(Round2,sum)\ncellStats(Round3,sum)\n```",
    "created" : 1461947273391.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1123495725",
    "id" : "CF7183AD",
    "lastKnownWriteTime" : 1442359924,
    "path" : "D:/Google_Drive/Harper_REU_VW2015/Modelling_Abundance_Occupancy_PointCount.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}